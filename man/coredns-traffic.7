.\" Generated by Mmark Markdown Processer - mmark.miek.nl
.TH "COREDNS-TRAFFIC" 7 "September 2021" "CoreDNS" "CoreDNS Plugins"

.SH "NAME"
.PP
\fItraffic\fP - handout addresses according to assignments from Envoy's xDS.

.SH "DESCRIPTION"
.PP
The \fItraffic\fP plugin is a balancer that allows traffic steering, weighted responses and draining
of clusters. A cluster is defined as: "A group of logically similar endpoints that Envoy
connects to." Each cluster has a name, which \fItraffic\fP extends to be a domain name. See "Naming
Clusters" below.

.PP
The use case for this plugin is when a cluster has endpoints running in multiple (e.g. Kubernetes)
clusters and you need to steer traffic to (or away) from these endpoints, i.e. endpoint A needs to
be upgraded, so all traffic to it is drained. Or the entire Kubernetes needs to upgraded, and \fIall\fP
endpoints need to be drained from it.

.PP
The cluster information is retrieved from a service discovery manager that implements the service
discovery protocols from Envoy
\[la]https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol\[ra].
It connects to the manager using the Aggregated Discovery Service (ADS) protocol. Endpoints and
clusters are discovered every 10 seconds. The plugin hands out responses that adhere to these
assignments. Only endpoints that are \fIhealthy\fP are handed out.

.PP
Note that the manager \fIitself\fP is also a cluster that is managed \fIby the management server\fP. This is
the \fImanagement cluster\fP (see \fB\fCcluster\fR below in "Syntax"). By default the name for cluster is \fB\fCxds\fR.
When bootstrapping \fItraffic\fP tries to retrieve the cluster endpoints for the management cluster,
when the cluster is not found \fItraffic\fP will return a fatal error.

.PP
The \fItraffic\fP plugin handles A, AAAA, SRV and TXT queries. TXT queries are purely used for debugging
as health status of the endpoints is ignored in that case.
Queries for non-existent clusters get a NXDOMAIN, where the minimal TTL is also set to 5s.

.PP
For A and AAAA queries each DNS response contains a single IP address that's considered the best
one. The TTL on these answer is set to 5s. It will only return successful responses either with an
answer or, otherwise, a NODATA response.

.PP
TXT replies will use the SRV record format augmented with the health status of each backend, as this
is useful for debugging.

.PP
.RS

.nf
web.lb.example.org.    5    IN    TXT    "100" "100" "18008" "endpoint\-0.web.lb.example.org." "HEALTHY"

.fi
.RE

.PP
For SRV queries \fIall\fP healthy backends will be returned - assuming the client doing the query
is smart enough to select the best one. When SRV records are returned, the endpoint DNS names
are synthesized \fB\fCendpoint-<N>.<cluster>.<zone>\fR that carries the IP address. Querying for these
synthesized names works as well.

.PP
\fITraffic\fP implements version 2 of the xDS API. It works with the management server as written in
https://github.com/miekg/xds
\[la]https://github.com/miekg/xds\[ra].

.SH "SYNTAX"
.PP
.RS

.nf
traffic TO...

.fi
.RE

.PP
This enabled the \fItraffic\fP plugin, with a default node ID of \fB\fCcoredns\fR and no TLS.

.IP \(bu 4
\fBTO...\fP are the control plane endpoints to bootstrap from. These must start with \fB\fCgrpc://\fR. The
port number defaults to 443, if not specified. These endpoints will be tried in the order given.


.PP
The extended syntax is available if you want more control.

.PP
.RS

.nf
traffic TO... {
    cluster CLUSTER
    id ID
    tls CERT KEY CA
    tls\_servername NAME
}

.fi
.RE

.IP \(bu 4
\fB\fCcluster\fR \fBCLUSTER\fP define the name of the management cluster. By default this is \fB\fCxds\fR.
.IP \(bu 4
\fB\fCid\fR \fBID\fP is how \fItraffic\fP identifies itself to the control plane. This defaults to \fB\fCcoredns\fR.
.IP \(bu 4
\fB\fCtls\fR \fBCERT\fP \fBKEY\fP \fBCA\fP define the TLS properties for gRPC connection. If this is omitted
an insecure connection is attempted. From 0 to 3 arguments can be provided with the meaning as
described below

.RS
.IP \(en 4
\fB\fCtls\fR - no client authentication is used, and the system CAs are used to verify the server
certificate
.IP \(en 4
\fB\fCtls\fR \fBCA\fP - no client authentication is used, and the file CA is used to verify the
server certificate
.IP \(en 4
\fB\fCtls\fR \fBCERT\fP \fBKEY\fP - client authentication is used with the specified cert/key pair. The
server certificate is verified with the system CAs.
.IP \(en 4
\fB\fCtls\fR \fBCERT\fP \fBKEY\fP \fBCA\fP - client authentication is used with the specified cert/key
pair. The server certificate is verified using the specified CA file.

.RE
.IP \(bu 4
\fB\fCtls_servername\fR \fBNAME\fP allows you to set a server name in the TLS configuration. This is
needed because \fItraffic\fP connects to an IP address, so it can't infer the server name from it.


.SH "NAMING CLUSTERS"
.PP
When a cluster is named this usually consists out of a single word, i.e. "cluster-v0", or "web".
The \fItraffic\fP plugins uses the name(s) specified in the Server Block to create fully qualified
domain names. For example if the Server Block specifies \fB\fClb.example.org\fR as one of the names,
and "cluster-v0" is one of the load balanced cluster, \fItraffic\fP will respond to queries asking for
\fB\fCcluster-v0.lb.example.org.\fR and the same goes for "web"; \fB\fCweb.lb.example.org\fR.

.PP
For SRV queries all endpoints are returned, the SRV target names are synthesized:
\fB\fCendpoint-<N>.web.lb.example.org\fR to take the example from above. \fIN\fP is an integer starting with 0.

.SH "MATCHING ALGORITHM"
.PP
How are queries match against the data we receive from xDS endpoint?

.IP 1\. 4
Does the cluster exist? If not return NXDOMAIN, otherwise continue.
.IP 2\. 4
Run through the endpoints, discard any endpoints that are not HEALTHY. If we are left with no
endpoint return a NODATA response, otherwise continue.
.IP 3\. 4
If weights are assigned, use those to pick an endpoint, otherwise randomly pick one and return a
response to the client. Weights are copied from the xDS data, priority is not used and set to 0
for all SRV records. Note that weights in SRV records are 16 bits, but xDS uses uint32; you have
been warned.


.SH "METRICS"
.PP
If monitoring is enabled (via the \fIprometheus\fP plugin) then the following metric are exported:

.IP \(bu 4
\fB\fCcoredns_traffic_clusters_tracked{}\fR the number of tracked clusters.
.IP \(bu 4
\fB\fCcoredns_traffic_endpoints_tracked{}\fR the number of tracked endpoints.


.SH "READY"
.PP
This plugin report readiness to the \fIready\fP plugin. This will happen after a gRPC stream has been
established to the control plane.

.SH "EXAMPLES"
.PP
.RS

.nf
lb.example.org {
    traffic grpc://127.0.0.1:18000 {
        id test\-id
    }
    debug
    log
}

.fi
.RE

.PP
This will load balance any names under \fB\fClb.example.org\fR using the data from the manager running on
localhost on port 18000. The node ID will be \fB\fCtest-id\fR and no TLS will be used. Assuming a
management server returns config for \fB\fCweb\fR cluster, you can query CoreDNS for it, below we do an
address lookup, which returns an address for the endpoint. The second example shows a SRV lookup
which returns all endpoints.

.PP
.RS

.nf
$ dig web.lb.example.org +noall +answer
web.lb.example.org.    5    IN    A    127.0.1.1

$ dig web.lb.example.org SRV +noall +answer +additional
web.lb.example.org.    5    IN    SRV    100 100 18008 endpoint\-0.web.lb.example.org.
web.lb.example.org.    5    IN    SRV    100 100 18008 endpoint\-1.web.lb.example.org.
web.lb.example.org.    5    IN    SRV    100 100 18008 endpoint\-2.web.lb.example.org.

endpoint\-0.web.lb.example.org. 5 IN    A    127.0.1.1
endpoint\-1.web.lb.example.org. 5 IN    A    127.0.1.2
endpoint\-2.web.lb.example.org. 5 IN    A    127.0.2.1

.fi
.RE

.SH "BUGS"
.PP
Credentials are not implemented. Bootstrapping is not fully implemented, \fItraffic\fP will connect to
the first working \fBTO\fP address, but then stops short of re-connecting to the endpoints of the
management \fBCLUSTER\fP.

.PP
Load reporting is not supported for the following reason: A DNS query is done by a resolver.
Behind this resolver (which can also cache) there may be many clients that will use this reply. The
responding server (CoreDNS) has no idea how many clients use this resolver. So reporting a load of
+1 on the CoreDNS side can results in anything from 1 to 1000+ of queries on the endpoint, making
the load reporting from the \fItraffic\fP plugin highly inaccurate. Hence it is not done.

.SH "ALSO SEE"
.PP
A Envoy management server and command line interface can be found on
GitHub
\[la]https://github.com/miekg/xds\[ra].

